{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Aerial Cactus Identification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Determine whether an image contains a columnar cactus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sorting the data into classes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0017c3c18ddd57a2ea6f9848c79d83d2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002134abf28af54575c18741b89dd2a4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0024320f43bdd490562246435af4f90b.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>002930423b9840e67e5a54afd4768a1e.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00351838ebf6dff6e53056e00a1e307c.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
       "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
       "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
       "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
       "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n",
       "5  0017c3c18ddd57a2ea6f9848c79d83d2.jpg           1\n",
       "6  002134abf28af54575c18741b89dd2a4.jpg           0\n",
       "7  0024320f43bdd490562246435af4f90b.jpg           0\n",
       "8  002930423b9840e67e5a54afd4768a1e.jpg           1\n",
       "9  00351838ebf6dff6e53056e00a1e307c.jpg           1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(base_url + 'train.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = df.id\n",
    "has_cactus = df.has_cactus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(base_url + 'train')\n",
    "if \"has_cactus\" not in folders:\n",
    "    os.mkdir(base_url + 'train/has_cactus')\n",
    "if \"no_cactus\" not in folders:\n",
    "    os.mkdir(base_url + 'train/no_cactus')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {image_name: target for image_name, target in zip(image_names, has_cactus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsorted_images = os.listdir(base_url + 'train')\n",
    "unsorted_images = [i for i in unsorted_images if i[-4:] == \".jpg\"]\n",
    "for i in unsorted_images:\n",
    "    target = data[i]\n",
    "    source = base_url + 'train/' + i\n",
    "    if target == 1:\n",
    "        destination = base_url + 'train/has_cactus/' + i\n",
    "        shutil.move(source, destination)\n",
    "    else:\n",
    "        destination = base_url + 'train/no_cactus/' + i\n",
    "        shutil.move(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsorted_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data visualisation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images_with_cactus = len(os.listdir(base_url + 'test/has_cactus/'))\n",
    "total_images_without_cactus = len(os.listdir(base_url + 'test/no_cactus/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar([\"Has cactus\", \"No cactus\"], [total_images_with_cactus, total_images_without_cactus])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training the CNN network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(8*8*32, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root='dataset/train/',\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    dataset_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return dataset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/274], Loss: 0.2499, Accuracy: 93.75%\n",
      "Epoch [1/10], Step [200/274], Loss: 0.1748, Accuracy: 89.06%\n",
      "Epoch [2/10], Step [100/274], Loss: 0.0844, Accuracy: 95.31%\n",
      "Epoch [2/10], Step [200/274], Loss: 0.1003, Accuracy: 95.31%\n",
      "Epoch [3/10], Step [100/274], Loss: 0.1137, Accuracy: 95.31%\n",
      "Epoch [3/10], Step [200/274], Loss: 0.0464, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [100/274], Loss: 0.0213, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [200/274], Loss: 0.0236, Accuracy: 98.44%\n",
      "Epoch [5/10], Step [100/274], Loss: 0.0550, Accuracy: 98.44%\n",
      "Epoch [5/10], Step [200/274], Loss: 0.0356, Accuracy: 98.44%\n",
      "Epoch [6/10], Step [100/274], Loss: 0.0474, Accuracy: 98.44%\n",
      "Epoch [6/10], Step [200/274], Loss: 0.2187, Accuracy: 95.31%\n",
      "Epoch [7/10], Step [100/274], Loss: 0.0184, Accuracy: 98.44%\n",
      "Epoch [7/10], Step [200/274], Loss: 0.0183, Accuracy: 98.44%\n",
      "Epoch [8/10], Step [100/274], Loss: 0.0135, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [200/274], Loss: 0.0153, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [100/274], Loss: 0.0612, Accuracy: 96.88%\n",
      "Epoch [9/10], Step [200/274], Loss: 0.0168, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [100/274], Loss: 0.0375, Accuracy: 98.44%\n",
      "Epoch [10/10], Step [200/274], Loss: 0.0009, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "dataset = load_dataset()\n",
    "batches = len(dataset)\n",
    "\n",
    "for j in range(0, epochs):\n",
    "    for i, (images, labels) in enumerate(dataset):\n",
    "        outputs = model(images)\n",
    "        error = loss(outputs, labels)\n",
    "        loss_history.append(error.item())\n",
    "        \n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy_history.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(j + 1, epochs, i + 1, batches, error.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluating the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.5657142857143 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predicting for Kaggle submission</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submission_dataset():\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root='dataset/test',\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    dataset_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return dataset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in load_submission_dataset():\n",
    "        outputs = model(images)\n",
    "        _, result = torch.max(outputs.data, 1)\n",
    "        predictions.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "image_id = os.listdir(base_url + 'test/prediction')\n",
    "submission = pd.DataFrame({'id':image_id, 'has_cactus':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
